{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable,gradcheck\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.append('../scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoader:\n",
    "    def __init__(self, path):\n",
    "        with open(path, 'r') as f:\n",
    "            self.data = f.read().splitlines()\n",
    "        split_tab = lambda x: [item.split('\\t') for item in x]\n",
    "        self.data = split_tab(self.data)\n",
    "        self.num_data = len(self.data)\n",
    "        self.arr = np.array(self.data)\n",
    "        self.df = pd.DataFrame({\n",
    "            'start': self.arr[:, 0],\n",
    "            'relation': self.arr[:, 1],\n",
    "            'dest': self.arr[:, 2]\n",
    "        })\n",
    "        \n",
    "        # get one_hot relation\n",
    "        one_hot_relation = [[item[1],] for item in self.data]\n",
    "        self.enc = OneHotEncoder().fit(one_hot_relation)\n",
    "        self.one_hot_relation = self.enc.transform(one_hot_relation).toarray()\n",
    "        self.y_index = np.argmax(self.one_hot_relation, axis=1)\n",
    "    \n",
    "    def encode_index(self):\n",
    "        start = self.arr[:, 0].astype(np.uint32)\n",
    "        end = self.arr[:, 2].astype(np.uint32)\n",
    "        unpack_bits = lambda num, m: np.array(list(np.binary_repr(num).zfill(m))).astype(np.uint8)\n",
    "        m = int(np.log2(max(np.max(start), np.max(end)))) + 1\n",
    "        start = np.stack([unpack_bits(i, m) for i in start])\n",
    "        end = np.stack([unpack_bits(i, m) for i in end])\n",
    "        return np.concatenate((start, end), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training: 86835\n",
      "Number of Test: 3134\n",
      "Number of Validation: 3034\n",
      "\n",
      "Number of Total Relations: 11\n",
      "Training Relations: 11\n",
      "Valiation Relations: 11\n",
      "Test Relations: 11\n",
      "\n",
      "Number of Total Nodes: 40943\n",
      "Training Nodes: 40559\n",
      "Valiation Nodes: 5173\n",
      "Test Nodes: 5323\n"
     ]
    }
   ],
   "source": [
    "verbose = False\n",
    "data_path = '../datasets/WN18RR/'    \n",
    "\n",
    "# datasets\n",
    "train = DatasetLoader(data_path + 'train.txt')\n",
    "test = DatasetLoader(data_path + 'test.txt')\n",
    "val = DatasetLoader(data_path + 'valid.txt')\n",
    "\n",
    "# get uniques\n",
    "relations_all = np.concatenate((train.arr[:, 1], val.arr[:, 1], test.arr[:, 1]))\n",
    "relations = np.unique(relations_all)\n",
    "nodes_all = np.concatenate((train.arr[:, (0, 2)], val.arr[:, (0, 2)], test.arr[:, (0, 2)]))\n",
    "nodes = np.unique(nodes_all)\n",
    "\n",
    "print('Number of Training:', len(train.arr))\n",
    "print('Number of Test:', len(test.arr))\n",
    "print('Number of Validation:', len(val.arr))\n",
    "print()\n",
    "print('Number of Total Relations:', len(relations))\n",
    "print('Training Relations:', len(np.unique(train.arr[:, 1])))\n",
    "print('Valiation Relations:', len(np.unique(val.arr[:, 1])))\n",
    "print('Test Relations:', len(np.unique(test.arr[:, 1])))\n",
    "print()\n",
    "print('Number of Total Nodes:', len(nodes))\n",
    "print('Training Nodes:', len(np.unique(train.arr[:, (0, 2)])))\n",
    "print('Valiation Nodes:', len(np.unique(val.arr[:, (0, 2)])))\n",
    "print('Test Nodes:', len(np.unique(test.arr[:, (0, 2)])))\n",
    "if verbose:\n",
    "    print('Unique Relations:', relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.encode_index()\n",
    "X_val = val.encode_index()\n",
    "X_test = test.encode_index()\n",
    "\n",
    "y_train = train.y_index\n",
    "y_val = val.y_index\n",
    "y_test = test.y_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.01 s, sys: 540 ms, total: 2.55 s\n",
      "Wall time: 2.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from DNN import DNN\n",
    "\n",
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:1')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# model settings\n",
    "D_in  = X_train.shape[1]\n",
    "D_out = len(relations)\n",
    "H = 200\n",
    "Depth = 5\n",
    "NUMEPOCHS = 1000\n",
    "Batch_size = 2000\n",
    "\n",
    "model = DNN(\n",
    "    input_size=D_in,\n",
    "    output_size=D_out,\n",
    "    hidden_size=H,\n",
    "    depth=Depth\n",
    ").to(device)\n",
    "\n",
    "# Loss Function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adamax(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([86835, 48])\n",
      "torch.Size([86835])\n",
      "torch.Size([3034, 48])\n",
      "torch.Size([3034])\n",
      "torch.Size([3134, 48])\n",
      "torch.Size([3134])\n"
     ]
    }
   ],
   "source": [
    "# Input Data\n",
    "trainX = Variable(torch.from_numpy(X_train).float())\n",
    "trainY = Variable(torch.from_numpy(y_train).long())\n",
    "valX = Variable(torch.from_numpy(X_val).float())\n",
    "valY = Variable(torch.from_numpy(y_val).long())\n",
    "testX = Variable(torch.from_numpy(X_test).float())\n",
    "testY = Variable(torch.from_numpy(y_test).long())\n",
    "\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "print(valX.shape)\n",
    "print(valY.shape)\n",
    "print(testX.shape)\n",
    "print(testY.shape)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    trainX = trainX.to(device)\n",
    "    trainY = trainY.to(device)\n",
    "    testX = testX.to(device)\n",
    "    testY = testY.to(device)\n",
    "    valX = valX.to(device)\n",
    "    valY = valY.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs\n",
      "0, tensor(1.4281, device='cuda:1', grad_fn=<NllLossBackward>)\n",
      "100, tensor(0.3836, device='cuda:1', grad_fn=<NllLossBackward>)\n",
      "200, tensor(0.0602, device='cuda:1', grad_fn=<NllLossBackward>)\n",
      "300, tensor(0.0209, device='cuda:1', grad_fn=<NllLossBackward>)\n",
      "400, tensor(0.0088, device='cuda:1', grad_fn=<NllLossBackward>)\n",
      "500, tensor(0.0144, device='cuda:1', grad_fn=<NllLossBackward>)\n",
      "600, tensor(0.0074, device='cuda:1', grad_fn=<NllLossBackward>)\n",
      "700, tensor(0.0050, device='cuda:1', grad_fn=<NllLossBackward>)\n",
      "800, tensor(0.0017, device='cuda:1', grad_fn=<NllLossBackward>)\n",
      "900, tensor(0.0028, device='cuda:1', grad_fn=<NllLossBackward>)\n",
      "\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Epochs\")\n",
    "for epoch in range(NUMEPOCHS):\n",
    "    data_train_loader = DataLoader(\n",
    "        list(zip(trainX,trainY)), \n",
    "        batch_size=Batch_size, \n",
    "        shuffle=True\n",
    "    )\n",
    "    for batchX, batchY in data_train_loader: \n",
    "        # Forward pass\n",
    "        outputs = model(batchX)\n",
    "        loss = criterion(outputs, batchY)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()     \n",
    "        optimizer.step()\n",
    "        \n",
    "    # Print Epochs and Losses to Monitor Convergence\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"{}\".format(epoch),end = \", \")\n",
    "        print(loss)\n",
    "        \n",
    "print('\\nTraining Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import label_ranking_loss, label_ranking_average_precision_score, coverage_error\n",
    "def evaluate(X, Y, name):\n",
    "    score = model(X).detach().cpu().numpy()\n",
    "    pred = np.argmax(score, axis=1)\n",
    "    score = (\n",
    "        score - np.min(score, axis=1)[:, np.newaxis]\n",
    "    ) / (\n",
    "        np.max(score, axis=1) - np.min(score, axis=1)\n",
    "    )[:, np.newaxis]\n",
    "    \n",
    "    true = Y.cpu().numpy()\n",
    "    true_score = np.zeros_like(score)\n",
    "    true_score[np.arange(len(true_score)), true] += 1\n",
    "    \n",
    "    hit_3 = score.argsort(axis=1)[np.arange(len(score)), -3:]\n",
    "    hit_10 = score.argsort(axis=1)[np.arange(len(score)), -10:]\n",
    "    hit_3 = np.sum(np.max(true[:, np.newaxis] == hit_3, axis=1)) / len(score)\n",
    "    hit_10 = np.sum(np.max(true[:, np.newaxis] == hit_10, axis=1)) / len(score)\n",
    "    \n",
    "    print(name + ' Accuracy:\\t', accuracy_score(true, pred))\n",
    "    print(name + ' F1-micro:\\t', f1_score(true, pred, average='micro'))\n",
    "    print(name + ' F1-macro:\\t', f1_score(true, pred, average='macro'))\n",
    "    print(name + ' MRR:\\t', label_ranking_average_precision_score(true_score, score))\n",
    "    print(name + ' Ranking Loss:\\t', label_ranking_loss(true_score, score))\n",
    "    print(name + ' Coverage Error:\\t', coverage_error(true_score, score))\n",
    "    print(name + ' Hit@1:\\t', accuracy_score(true, pred))\n",
    "    print(name + ' Hit@3:\\t', hit_3)\n",
    "    print(name + ' Hit@10:\\t', hit_10)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Labels :\n",
      " [ 7 10  6 ...  1  2  3]\n",
      "Test Predictions :\n",
      " [7 1 6 ... 1 3 3]\n",
      "\n",
      "Training Accuracy:\t 0.9980192318765475\n",
      "Training F1-micro:\t 0.9980192318765475\n",
      "Training F1-macro:\t 0.9938338234599562\n",
      "Training MRR:\t 0.9990096159382738\n",
      "Training Ranking Loss:\t 0.00019807681234525247\n",
      "Training Coverage Error:\t 1.0019807681234525\n",
      "Training Hit@1:\t 0.9980192318765475\n",
      "Training Hit@3:\t 1.0\n",
      "Training Hit@10:\t 1.0\n",
      "\n",
      "Validation Accuracy:\t 0.8592617007251153\n",
      "Validation F1-micro:\t 0.8592617007251154\n",
      "Validation F1-macro:\t 0.653897068489025\n",
      "Validation MRR:\t 0.9160120486340003\n",
      "Validation Ranking Loss:\t 0.02633487145682268\n",
      "Validation Coverage Error:\t 1.2633487145682267\n",
      "Validation Hit@1:\t 0.8592617007251153\n",
      "Validation Hit@3:\t 0.9706657877389585\n",
      "Validation Hit@10:\t 1.0\n",
      "\n",
      "Test Accuracy:\t 0.8611997447351627\n",
      "Test F1-micro:\t 0.8611997447351627\n",
      "Test F1-macro:\t 0.6670316113069182\n",
      "Test MRR:\t 0.9168517843214722\n",
      "Test Ranking Loss:\t 0.02597319719208679\n",
      "Test Coverage Error:\t 1.259731971920868\n",
      "Test Hit@1:\t 0.8611997447351627\n",
      "Test Hit@3:\t 0.9693682195277601\n",
      "Test Hit@10:\t 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Test Labels :\\n', testY.cpu().numpy())\n",
    "print('Test Predictions :\\n', np.argmax(model(testX).detach().cpu().numpy(), axis=1))\n",
    "print()\n",
    "\n",
    "evaluate(trainX, trainY, 'Training')\n",
    "evaluate(valX, valY, 'Validation')\n",
    "evaluate(testX, testY, 'Test')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
